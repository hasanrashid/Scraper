# GreenFlare-like PDF Site Crawler

A comprehensive website crawler for discovering all PDF documents with intelligent title and author extraction, similar to GreenFlare DB functionality.

## ğŸš€ Key Features

### **Comprehensive Site Crawling**
- ğŸŒ **Full Website Crawling**: Systematically crawls entire websites to discover all PDF documents
- ğŸ—ºï¸ **Sitemap Discovery**: Automatically finds and parses XML sitemaps for efficient crawling
- ğŸ¤– **Robots.txt Compliance**: Respects robots.txt rules and crawling guidelines
- ğŸ¯ **Configurable Depth**: Set maximum crawl depth and page limits
- âš¡ **Rate Limiting**: Built-in delays to respect server resources

### **Intelligent PDF Discovery**
- ğŸ“„ **Smart PDF Detection**: Identifies PDF links through multiple methods (URL patterns, content-type, query parameters)
- ğŸ” **Metadata Extraction**: Automatically extracts title, author, file size, and other metadata
- ğŸ“Š **File Validation**: Checks file sizes and validates PDF accessibility
- ğŸŒ **Domain Analysis**: Tracks which domains host the most PDFs

### **Advanced Title & Author Extraction**
- ğŸ“ **Title Intelligence**: Extracts titles from filenames, link text, and context
- ğŸ‘¤ **Author Detection**: Uses regex patterns to identify author names in URLs and filenames
- ğŸ§¹ **Content Cleaning**: Removes common unwanted phrases like "Download PDF", "Click here"
- ğŸ¯ **Context Analysis**: Examines surrounding text for better metadata understanding

### **Comprehensive CSV Export**
- ğŸ“Š **Rich Metadata**: Exports 14 columns of detailed information per PDF
- ğŸŒ **Discovery Context**: Tracks source pages, link text, and discovery timestamps
- ğŸ“ˆ **Statistical Summary**: Provides crawl statistics and domain distribution
- ğŸ’¾ **UTF-8 Encoding**: Properly handles international characters

## ğŸ“Š CSV Export Format

Each discovered PDF is exported with comprehensive metadata:

| Column | Description | Example |
|--------|-------------|---------|
| **URL** | Direct PDF download link | `https://university.edu/research/paper.pdf` |
| **Title** | Extracted document title | `Machine Learning Fundamentals` |
| **Author** | Extracted author name | `Dr. Jane Smith` |
| **File Size (Bytes)** | Exact file size | `2097152` |
| **File Size (MB)** | Human-readable size | `2.00` |
| **Content Type** | HTTP content type | `application/pdf` |
| **Last Modified** | Server modification date | `Wed, 15 Nov 2023 10:30:00 GMT` |
| **Discovered On Page** | Source page URL | `https://university.edu/publications/` |
| **Discovery Date** | When PDF was found | `2024-01-15 14:22:33` |
| **Response Code** | HTTP status code | `200` |
| **Depth** | Crawl depth level | `2` |
| **Link Text** | Original anchor text | `Download Research Paper (PDF)` |
| **Link Context** | Surrounding page text | `Latest research findings in...` |
| **Domain** | PDF hosting domain | `university.edu` |

## ğŸ› ï¸ Installation & Setup

### 1. Prerequisites
```bash
cd /home/soaad/Documents/Scraper
pip install -r requirements.txt
```

### 2. Configuration
Edit `Configuration/config.ini` to customize crawling behavior:

```ini
[SiteCrawler]
# Maximum pages to crawl per site
max-pages-per-site=1000
# Maximum crawl depth
max-crawl-depth=10
# Delay between requests (seconds)
request-delay=1.0
# Follow external links
follow-external-links=false
# Extract PDF metadata
extract-pdf-content=true
# Minimum PDF file size in KB
min-pdf-size-kb=50
# Maximum PDF file size in MB
max-pdf-size-mb=100

[Filenames]
# Output files
pdf-documents-csv=pdf_documents.csv
sitemap-urls=sitemap_urls.txt
```

## ğŸš€ Usage

### Command Line Interface

**Basic crawling:**
```bash
python greenflare_crawler.py https://university.edu
```

**Advanced options:**
```bash
python greenflare_crawler.py https://research-site.org \
  --max-pages 500 \
  --max-depth 5 \
  --output my_pdfs.csv \
  --delay 2.0 \
  --follow-external
```

**Available options:**
- `--max-pages N`: Maximum pages to crawl (default: from config)
- `--max-depth N`: Maximum crawl depth (default: from config)  
- `--follow-external`: Follow links to external domains
- `--output FILE`: Custom output CSV file path
- `--delay SECONDS`: Delay between requests

### Programmatic Usage

```python
from Core.config_manager import IniConfigManager
from Core.http_client import RequestsHttpClient
from Core.scraper import Scraper
from Core.pdf_site_crawler import PDFSiteCrawler

# Initialize components
config_manager = IniConfigManager()
http_client = RequestsHttpClient(config_manager.get_user_agent())
scraper = Scraper(config_manager, http_client)
crawler = PDFSiteCrawler(config_manager, http_client, scraper)

# Crawl entire site
pdfs = crawler.crawl_site(
    start_url="https://research-university.edu",
    max_pages=1000,
    max_depth=8,
    follow_external=False
)

# Export results
csv_file = crawler.export_to_csv("university_pdfs.csv")
print(f"Found {len(pdfs)} PDFs, exported to {csv_file}")

# Get detailed statistics
summary = crawler.get_crawl_summary()
print(f"Crawled {summary['crawl_stats']['pages_crawled']} pages")
print(f"Total PDF size: {summary['total_size_mb']:.2f} MB")
print(f"PDFs with authors: {summary['pdfs_with_authors']}")
```

## ğŸ“ˆ Output Examples

### Console Output
```
ğŸŒ GreenFlare-like PDF Site Crawler
==================================================
Target URL: https://university.edu

ğŸ“‹ Initializing crawler components...

ğŸ” Starting comprehensive site crawl...

âœ… Crawl completed!

ğŸ“š Found 47 PDF documents:

1. Advanced Machine Learning Techniques
   ğŸ“„ URL: https://university.edu/cs/papers/ml-advanced.pdf
   ğŸ‘¤ Author: Dr. Sarah Johnson
   ğŸ“Š Size: 3.45 MB
   ğŸŒ Found on: https://university.edu/cs/publications/
   ğŸ”— Link text: Download Full Research Paper (PDF)

2. Quantum Computing Fundamentals
   ğŸ“„ URL: https://university.edu/physics/quantum-basics.pdf
   ğŸ‘¤ Author: Prof. Michael Chen
   ğŸ“Š Size: 2.12 MB
   ğŸŒ Found on: https://university.edu/physics/courses/
   ğŸ”— Link text: Course Materials - Quantum Computing

... and 45 more PDFs

ğŸ’¾ Exported all PDFs to: pdf_documents.csv

ğŸ“Š Crawl Statistics:
   ğŸƒ Duration: 157.3 seconds
   ğŸ“„ Pages crawled: 284
   âš¡ Pages/second: 1.81
   ğŸ“š PDFs found: 47
   ğŸ’½ Total size: 156.78 MB
   ğŸ“ PDFs with authors: 31
   ğŸŒ Unique domains: 1

ğŸŒ PDFs by domain:
   university.edu: 47 PDFs

ğŸ“Š Largest PDFs:
   8.95 MB - Computer Vision Research Compendium
   6.23 MB - Statistical Methods in Data Science
   5.67 MB - Neural Networks Architecture Guide

ğŸ¯ All PDF metadata has been saved to: pdf_documents.csv
```

### CSV Output Preview
```csv
URL,Title,Author,File Size (Bytes),File Size (MB),Content Type,Last Modified,Discovered On Page,Discovery Date,Response Code,Depth,Link Text,Link Context,Domain
https://university.edu/cs/ml-paper.pdf,Machine Learning Advances,Dr. Sarah Johnson,3618816,3.45,application/pdf,Wed 15 Nov 2023 10:30:00 GMT,https://university.edu/cs/publications/,2024-01-15 14:22:33,200,2,Download Research Paper (PDF),"Latest research findings in machine learning...",university.edu
https://university.edu/physics/quantum.pdf,Quantum Computing Basics,Prof. Michael Chen,2225152,2.12,application/pdf,Mon 13 Nov 2023 15:45:00 GMT,https://university.edu/physics/courses/,2024-01-15 14:23:15,200,3,Course Materials - Quantum Computing,"Fundamental concepts in quantum mechanics...",university.edu
```

## ğŸ¯ Use Cases

Perfect for discovering PDFs from:

### **Academic Institutions**
- ğŸ“š University digital libraries
- ğŸ”¬ Research publication repositories  
- ğŸ“– Course material collections
- ğŸ“ Thesis and dissertation archives

### **Research Organizations**
- ğŸ§ª Scientific paper databases
- ğŸ“Š Technical report collections
- ğŸ“‹ Policy document repositories
- ğŸ“ˆ Statistical analysis reports

### **Corporate Websites**
- ğŸ“„ White paper collections
- ğŸ“‹ Annual reports and financials
- ğŸ”§ Technical documentation
- ğŸ“Š Product specifications and manuals

### **Government Portals**
- ğŸ›ï¸ Policy documents and legislation
- ğŸ“Š Statistical reports and data
- ğŸ“‹ Public consultation documents
- ğŸ—‚ï¸ Administrative guidance materials

## ğŸ”§ Advanced Features

### **Sitemap Integration**
- Automatically discovers sitemap.xml files
- Parses sitemap indexes and individual sitemaps
- Supports both XML and text format sitemaps
- Saves discovered URLs for reference

### **Robots.txt Compliance**
- Downloads and parses robots.txt files
- Respects crawl delays and disallow directives
- Gracefully handles sites without robots.txt
- Logs blocked URLs for transparency

### **Intelligent Link Following**
- Skips binary files (images, videos, executables)
- Avoids admin and login pages
- Filters out CSS, JavaScript, and other assets
- Configurable domain restrictions

### **Error Handling & Logging**
- Comprehensive error tracking and logging
- Failed URL collection and reporting
- Graceful handling of timeouts and connection errors
- Progress reporting every 10 pages

### **Performance Optimization**
- Configurable request delays for rate limiting
- Duplicate URL detection and skipping
- Memory-efficient crawling for large sites
- Interrupt handling for graceful shutdown

## ğŸ§ª Testing

Run the comprehensive test suite:

```bash
python -m unittest Tests.test_pdf_site_crawler -v
```

**Test coverage includes:**
- âœ… PDF document metadata creation and CSV export
- âœ… URL normalization and domain comparison
- âœ… Title and author extraction algorithms
- âœ… Link following and crawling logic
- âœ… Sitemap parsing (XML and text formats)
- âœ… File size validation and filtering
- âœ… Crawl statistics and summary generation

## âš™ï¸ Configuration Reference

### **Crawling Behavior**
```ini
[SiteCrawler]
max-pages-per-site=1000      # Stop after N pages
max-crawl-depth=10           # Maximum link depth to follow
request-delay=1.0            # Seconds between requests
follow-external-links=false  # Stay within original domain
```

### **PDF Filtering**
```ini
min-pdf-size-kb=50          # Skip very small files
max-pdf-size-mb=100         # Skip very large files  
extract-pdf-content=true    # Extract metadata when possible
```

### **Output Configuration**
```ini
[Filenames]
pdf-documents-csv=pdf_documents.csv  # Main output file
sitemap-urls=sitemap_urls.txt        # Discovered sitemap URLs
```

## ğŸš¦ Performance Guidelines

### **Recommended Settings by Site Size**

**Small sites (< 100 pages):**
```bash
--max-pages 100 --max-depth 5 --delay 0.5
```

**Medium sites (100-1000 pages):**  
```bash
--max-pages 500 --max-depth 7 --delay 1.0
```

**Large sites (1000+ pages):**
```bash
--max-pages 1000 --max-depth 10 --delay 2.0
```

### **Respectful Crawling**
- Use appropriate delays (1-2 seconds minimum)
- Respect robots.txt directives
- Monitor server response times
- Stop if receiving too many errors
- Consider crawling during off-peak hours

## ğŸ¤ Contributing

1. **Add new PDF detection patterns**: Extend `_is_pdf_url()` method
2. **Improve metadata extraction**: Enhance title/author regex patterns  
3. **Add new export formats**: Implement additional output formats beyond CSV
4. **Extend sitemap support**: Add support for sitemap index files
5. **Performance optimizations**: Add parallel processing or caching

## ğŸ“„ License

Same as the main Scraper project.

---

**Built for comprehensive PDF discovery across entire websites with enterprise-grade reliability and detailed metadata extraction.**